{
  "data": "data/corpus.txt",
  "val_data": null,
  "tokenizer": "data/tokenizer.model",
  "vocab_size": 32000,
  "block_size": 256,
  "n_layer": 3,
  "n_head": 8,
  "d_model": 256,
  "d_ff": 1024,
  "dropout": 0.1,
  "weight_tie": true,
  "batch_size": 12,
  "grad_accum": 20,
  "lr": 0.0003,
  "weight_decay": 0.1,
  "warmup_steps": 2000,
  "max_steps": 100000,
  "eval_interval": 1000,
  "eval_batches": 20,
  "checkpoint_dir": "ckpts",
  "resume": null,
  "num_workers": 2,
  "seed": 42,
  "save_interval": 1000,
  "chunk_bytes": 8388608
}
